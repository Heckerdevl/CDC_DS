{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6c20b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Advanced Tabular Ensemble...\n",
      "‚úÖ Fold 1 Complete\n",
      "‚úÖ Fold 2 Complete\n",
      "‚úÖ Fold 3 Complete\n",
      "‚úÖ Fold 4 Complete\n",
      "‚úÖ Fold 5 Complete\n",
      "\n",
      "--- Final Tabular R2: 0.82320 ---\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load your enriched dataset from the previous step\n",
    "df = pd.read_csv(\"transformed_tabular_v2.csv\")\n",
    "X = df.drop(columns=['log_price', 'lat', 'long'])\n",
    "y = df['log_price']\n",
    "\n",
    "def train_tabular_baseline(X, y):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    \n",
    "    print(\"üöÄ Training Advanced Tabular Ensemble...\")\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        \n",
    "        # HistGradientBoosting: Very fast, handles large datasets well\n",
    "        m1 = HistGradientBoostingRegressor(max_iter=1000, learning_rate=0.04, max_depth=10, random_state=101)\n",
    "        # LightGBM: Leaf-wise growth for higher precision\n",
    "        m2 = LGBMRegressor(n_estimators=1000, learning_rate=0.04, num_leaves=70, verbose=-1, random_state=101)\n",
    "        # CatBoost: Excellent at handling numerical and categorical relationships\n",
    "        m3 = CatBoostRegressor(iterations=1000, learning_rate=0.04, depth=8, verbose=0, random_seed=101)\n",
    "        \n",
    "        m1.fit(X_tr, y_tr)\n",
    "        m2.fit(X_tr, y_tr)\n",
    "        m3.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Blended Prediction\n",
    "        fold_preds = (0.3 * m1.predict(X_va)) + (0.35 * m2.predict(X_va)) + (0.35 * m3.predict(X_va))\n",
    "        oof_preds[va_idx] = fold_preds\n",
    "        print(f\"‚úÖ Fold {fold} Complete\")\n",
    "\n",
    "    score = r2_score(y, oof_preds)\n",
    "    return score, (m1, m2, m3)\n",
    "\n",
    "tabular_r2, base_models = train_tabular_baseline(X, y)\n",
    "print(f\"\\n--- Final Tabular R2: {tabular_r2:.5f} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9ac8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cpu\n",
      "üñºÔ∏è  Extracting features from 16209 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16209/16209 [13:39<00:00, 19.77it/s]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def extract_efficientnet_features(image_folder, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "    # Load Pre-trained EfficientNet-B0\n",
    "    # Weights.DEFAULT ensures we use the best available ImageNet weights\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "    \n",
    "    # Remove the final classification head to get the 1280-dimensional feature vector\n",
    "    model.classifier = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Image transformations: EfficientNet-B0 expects 224x224\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    features = []\n",
    "    # Sort files to ensure they match the order of your tabular dataframe\n",
    "    # Assuming files are named 'tile_id_0.jpg', 'tile_id_1.jpg', etc.\n",
    "    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n",
    "                        key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    print(f\"üñºÔ∏è  Extracting features from {len(image_files)} images...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_name in tqdm(image_files):\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Extract the 1280-dimensional embedding\n",
    "                feat = model(input_tensor)\n",
    "                features.append(feat.cpu().numpy().flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_name}: {e}\")\n",
    "                # Append zeros if image is corrupted to keep array alignment\n",
    "                features.append(np.zeros(1280))\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "visual_features_train = extract_efficientnet_features(\"property_visuals\")\n",
    "np.save(\"train_visual_embeddings.npy\", visual_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55158951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802efb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.5\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f786117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Tabular Ensemble...\n",
      "üìä Tabular Ensemble R2 Score: 0.82018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Prepare Data\n",
    "# Using the enriched features from your preprocessing step\n",
    "X_tab = pd.read_csv(\"transformed_tabular_v2.csv\").drop(columns=['log_price', 'lat', 'long'])\n",
    "y = pd.read_csv(\"transformed_tabular_v2.csv\")['log_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tab, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Initialize Models\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42)\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.03, max_depth=7, random_state=42)\n",
    "cat = CatBoostRegressor(iterations=1000, learning_rate=0.03, depth=7, verbose=0, random_seed=42)\n",
    "\n",
    "# 3. Create the Ensemble (Weighted Voting)\n",
    "# We give higher weight to CatBoost as it's typically the strongest on real estate data\n",
    "tabular_ensemble = VotingRegressor(\n",
    "    estimators=[('rf', rf), ('xgb', xgb), ('cat', cat)],\n",
    "    weights=[1, 2, 2]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training Tabular Ensemble...\")\n",
    "tabular_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluation\n",
    "tabular_preds = tabular_ensemble.predict(X_test)\n",
    "r2_tabular = r2_score(y_test, tabular_preds)\n",
    "print(f\"üìä Tabular Ensemble R2 Score: {r2_tabular:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64b1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Training Hybrid Multimodal Model...\n",
      "üñºÔ∏è Hybrid Multimodal R2 Score: 0.83545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load pre-extracted visual features\n",
    "# (Assuming visual_features_train was saved from the EfficientNet step)\n",
    "vis_embeddings = np.load(\"train_visual_embeddings.npy\")\n",
    "\n",
    "# 2. Dimensionality Reduction\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "vis_reduced = pca.fit_transform(vis_embeddings)\n",
    "\n",
    "# 3. Combine Tabular + Visual for the Hybrid dataset\n",
    "X_hybrid = np.hstack([X_tab.values, vis_reduced])\n",
    "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_hybrid, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Hybrid Regressor\n",
    "# We use a high-depth CatBoost to handle the high-dimensional fused space\n",
    "hybrid_model = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=8, verbose=0, random_seed=42)\n",
    "\n",
    "print(\"üß¨ Training Hybrid Multimodal Model...\")\n",
    "hybrid_model.fit(X_h_train, y_h_train)\n",
    "\n",
    "# 5. Evaluation\n",
    "hybrid_preds = hybrid_model.predict(X_h_test)\n",
    "r2_hybrid = r2_score(y_h_test, hybrid_preds)\n",
    "print(f\"üñºÔ∏è Hybrid Multimodal R2 Score: {r2_hybrid:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f68eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üèÜ FINAL BATTLE RESULTS\n",
      "Standard Tabular Ensemble: 0.82018\n",
      "Hybrid Visual + Tabular:   0.83545\n",
      "========================================\n",
      "‚ú® SUCCESS: Satellite imagery improves prediction accuracy.\n",
      "Action: Using Hybrid Pipeline for Test Data...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÜ FINAL BATTLE RESULTS\")\n",
    "print(f\"Standard Tabular Ensemble: {r2_tabular:.5f}\")\n",
    "print(f\"Hybrid Visual + Tabular:   {r2_hybrid:.5f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if r2_hybrid > r2_tabular:\n",
    "    print(\"‚ú® SUCCESS: Satellite imagery improves prediction accuracy.\")\n",
    "    print(\"Action: Using Hybrid Pipeline for Test Data...\")\n",
    "    \n",
    "    # Process test imagery from 'property_visuals_test'\n",
    "    # vis_test = extract_efficientnet_features(\"property_visuals_test\")\n",
    "    # vis_test_pca = pca.transform(vis_test)\n",
    "    # Combine and predict...\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  NOTICE: Tabular data alone is more efficient for this split.\")\n",
    "    print(\"Action: Using Tabular Ensemble for Test Data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d340df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load original raw data to ensure no useful columns are missing\n",
    "train_raw = pd.read_csv(\"train(1)(train(1)).csv\")\n",
    "\n",
    "def fallback_preprocessing(df):\n",
    "    X = df.copy()\n",
    "    \n",
    "    # 1. DO NOT DROP LAT/LONG - Location is 90% of real estate value\n",
    "    # 2. Convert 'date' to 'house_age_at_sale'\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X['age_at_sale'] = X['date'].dt.year - X['yr_built']\n",
    "    \n",
    "    # 3. Create a 'renovated' binary flag\n",
    "    X['is_renovated'] = (X['yr_renovated'] > 0).astype(int)\n",
    "    \n",
    "    # 4. Log Transform area columns (reduces outlier impact)\n",
    "    area_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
    "    for col in area_cols:\n",
    "        X[f'log_{col}'] = np.log1p(X[col])\n",
    "    \n",
    "    # 5. Drop the original versions of transformed columns + IDs\n",
    "    cols_to_drop = ['id', 'date', 'yr_built', 'yr_renovated'] + area_cols\n",
    "    X = X.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_final = fallback_preprocessing(train_raw.drop(columns=['price']))\n",
    "y_log = np.log1p(train_raw['price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_log, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90801cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Reliable Ensemble...\n",
      "‚úÖ Fallback Tabular R2: 0.90561\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize with \"Safe\" parameters\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, subsample=0.8, random_state=42)\n",
    "cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0, random_seed=42)\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth=15, random_state=42)\n",
    "\n",
    "print(\"üöÄ Training Reliable Ensemble...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "cat.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Blend: Giving more weight to CatBoost and XGBoost\n",
    "y_pred = (0.4 * xgb.predict(X_test)) + (0.4 * cat.predict(X_test)) + (0.2 * rf.predict(X_test))\n",
    "\n",
    "r2_tabular = r2_score(y_test, y_pred)\n",
    "print(f\"‚úÖ Fallback Tabular R2: {r2_tabular:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e89d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è  Fallback Hybrid R2: 0.90332\n",
      "üèÜ Tabular is superior. Using restored columns for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have EfficientNet features extracted as 'vis_embeddings'\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "vis_reduced = pca.fit_transform(vis_embeddings)\n",
    "\n",
    "# Combine with our 'Restored' Tabular features\n",
    "X_hybrid = np.hstack([X_final.values, vis_reduced])\n",
    "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_hybrid, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hybrid Model\n",
    "h_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0)\n",
    "h_model.fit(X_h_train, y_h_train)\n",
    "\n",
    "r2_hybrid = r2_score(y_h_test, h_model.predict(X_h_test))\n",
    "print(f\"üñºÔ∏è  Fallback Hybrid R2: {r2_hybrid:.5f}\")\n",
    "\n",
    "# Final Comparison\n",
    "if r2_hybrid > r2_tabular:\n",
    "    print(\"üèÜ Hybrid is superior. Predicting on 'property_visuals_test'...\")\n",
    "else:\n",
    "    print(\"üèÜ Tabular is superior. Using restored columns for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acae94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Training Tabular Ensemble (Fast Mode)...\n",
      "üìä Validation R2 Score: 0.90624\n",
      "üíæ Done! Final predictions saved: 'tabular_final_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. LOAD & CLEAN DATA\n",
    "def quick_preprocess(df, is_train=True):\n",
    "    X = df.copy()\n",
    "    X.columns = X.columns.str.strip()\n",
    "    \n",
    "    # Feature Engineering (The \"Safe\" Set)\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X['age_at_sale'] = X['date'].dt.year - X['yr_built']\n",
    "    X['is_renovated'] = (X['yr_renovated'] > 0).astype(int)\n",
    "    \n",
    "    # Log area columns\n",
    "    area_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
    "    for col in area_cols:\n",
    "        X[f'log_{col}'] = np.log1p(X[col])\n",
    "    \n",
    "    y = None\n",
    "    if is_train:\n",
    "        y = np.log1p(X['price'])\n",
    "        X = X.drop(columns=['price'])\n",
    "        \n",
    "    X = X.drop(columns=['id', 'date', 'yr_built', 'yr_renovated'] + area_cols)\n",
    "    return X, y\n",
    "\n",
    "# Load datasets\n",
    "train_raw = pd.read_csv(\"train(1)(train(1)).csv\")\n",
    "test_raw = pd.read_csv(\"test2(test(1)).csv\")\n",
    "\n",
    "X_final, y_log = quick_preprocess(train_raw, is_train=True)\n",
    "X_test_final, _ = quick_preprocess(test_raw, is_train=False)\n",
    "\n",
    "# 2. TRAIN THE WINNING ENSEMBLE\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_final, y_log, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"‚ö° Training Tabular Ensemble (Fast Mode)...\")\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=42).fit(X_train, y_train)\n",
    "cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0, random_seed=42).fit(X_train, y_train)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# 3. EVALUATE & PREDICT\n",
    "def get_blend(models, data):\n",
    "    m1, m2, m3 = models\n",
    "    return (0.4 * m1.predict(data)) + (0.4 * m2.predict(data)) + (0.2 * m3.predict(data))\n",
    "\n",
    "val_preds = get_blend((xgb, cat, rf), X_val)\n",
    "print(f\"üìä Validation R2 Score: {r2_score(y_val, val_preds):.5f}\")\n",
    "\n",
    "# 4. FINAL SUBMISSION\n",
    "final_log_preds = get_blend((xgb, cat, rf), X_test_final)\n",
    "final_prices = np.expm1(final_log_preds)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_raw['id'], 'predicted_price': final_prices})\n",
    "submission.to_csv(\"tabular_final_submission.csv\", index=False)\n",
    "print(\"üíæ Done! Final predictions saved: 'tabular_final_submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955c71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_env)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
